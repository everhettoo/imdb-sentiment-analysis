{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To build sentiment analysis based on dataset from: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:30.530212Z",
     "start_time": "2025-03-17T07:36:28.942281Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/everhett/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:33.368066Z",
     "start_time": "2025-03-17T07:36:30.538124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To download spacy corpus. Please enable one of the required.\n",
    "# For bash shell\n",
    "! python -m spacy download en_core_web_sm\n",
    "\n",
    "#  For windows\n",
    "# %% python -m spacy download en"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/everhett/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\r\n",
      "  warnings.warn(\r\n",
      "Collecting en-core-web-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m40.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore the dataset to understand the features before preparing for building the sentiment prediction models."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:33.478317Z",
     "start_time": "2025-03-17T07:36:33.476606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the datasets used in stages for building the models.\n",
    "raw_dataset_path = 'data/imdb-movie-review-kaggle-laskhmipathi.csv'\n",
    "\n",
    "# After encoding the classes to binary values (1: positive and 0: negative).\n",
    "encoded_dataset_path = 'data/1-imdb-movie-review-encoded.csv'\n",
    "\n",
    "# The final preprocessed files for building the models.\n",
    "processed_dataset_path = 'data/2-imdb-movie-review-processed.csv'"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:33.846898Z",
     "start_time": "2025-03-17T07:36:33.482513Z"
    }
   },
   "source": [
    "# Let's glance at the dataset.\n",
    "data = pd.read_csv(raw_dataset_path, encoding=\"ISO-8859-1\")\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:33.860791Z",
     "start_time": "2025-03-17T07:36:33.854027Z"
    }
   },
   "source": [
    "# The dataset has the following data types.\n",
    "data.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:33.884689Z",
     "start_time": "2025-03-17T07:36:33.879417Z"
    }
   },
   "source": [
    "# Let's investigate if the dataset has missing values.\n",
    "data.isnull().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:33.913967Z",
     "start_time": "2025-03-17T07:36:33.909988Z"
    }
   },
   "source": [
    "# Let's investigate the distribution sentiment class distribution.\n",
    "data['sentiment'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:34.031313Z",
     "start_time": "2025-03-17T07:36:33.958169Z"
    }
   },
   "source": [
    "# The dataset has well-balanced classes for positive and negative sentiment (1:1).\n",
    "# Let's visualize it in a bar chart.\n",
    "data['sentiment'].value_counts().plot.bar()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='sentiment'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying special character in the dataset\n",
    "We will explore the noise in the data before building the model in this section. \\\n",
    "We will remove the noise during the preprocessing later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:34.824704Z",
     "start_time": "2025-03-17T07:36:34.070181Z"
    }
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define a regex pattern for URLs\n",
    "url_pattern = r'https?://(?:www\\.)?\\S+'\n",
    "\n",
    "# Let's explore whether the dataset contains hyperlinks.\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    text_list = re.findall(url_pattern, review_text)\n",
    "    if len(text_list) > 0:\n",
    "        temp.append(text_list)\n",
    "\n",
    "print(f'Dataset contains {len(temp)} number of hyperlinks.')\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 119 number of hyperlinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                         [http://www.invocus.net)]\n",
       "1           [http://blog.myspace.com/locoformovies]\n",
       "2    [http://www.comingsoon.net/films.php?id=36310]\n",
       "3                     [http://tinyurl.com/znyyq<br]\n",
       "4              [http://imdb.com/name/nm0834754/bio]\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Character"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:36.023428Z",
     "start_time": "2025-03-17T07:36:34.845365Z"
    }
   },
   "source": [
    "# Define a regex pattern for special character\n",
    "special_character = r'[#,.\\-$!/()?%_√Ø¬ø¬Ω:&|;]'\n",
    "\n",
    "# Let's explore whether the dataset contains special character.\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    text_list = re.findall(special_character, review_text)\n",
    "    if len(text_list) > 0:\n",
    "        temp.append(text_list)\n",
    "\n",
    "print(f'Dataset contains {len(temp)} number of special characters.')\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 49992 number of special characters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [., ,, ., /, /, ,, ., ,, ., ,, ., ,, ., /, /, ...\n",
       "1    [., /, /, -, -, -, ,, ,, ., /, /, -, !, ,, ., ...\n",
       "2    [,, -, ., ,, (, ), ., :, ,, ., /, /, (, ?, ), ...\n",
       "3    [(, ), &, ., /, /, ., ., ., ,, ., /, /, ,, !, ...\n",
       "4    [., ., ., ,, ., /, /, ,, ., ,, ,, ., ,, ., ., ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:37.352125Z",
     "start_time": "2025-03-17T07:36:36.042211Z"
    }
   },
   "source": [
    "# Define a regex pattern for date\n",
    "date_pattern = r'\\d{1,4}[\\/\\-]\\d{1,2}[\\/\\-]\\d{1,4}'\n",
    "\n",
    "# Let's explore whether the dataset contains date.\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    text_list = re.findall(date_pattern, review_text)\n",
    "    if len(text_list) > 0:\n",
    "        temp.append(text_list)\n",
    "\n",
    "print(f'Dataset contains {len(temp)} number of date.')\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 209 number of date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [4/25/08]\n",
       "1        [7-1/2]\n",
       "2    [5/28/2000]\n",
       "3       [7/3/40]\n",
       "4    [6/20/2009]\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stand Alone Number"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:38.749903Z",
     "start_time": "2025-03-17T07:36:37.373203Z"
    }
   },
   "source": [
    "# Define a regex pattern for stand-alone number\n",
    "number= r'\\d+'\n",
    "\n",
    "# Let's explore whether the dataset stand-alone number.\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    text_list = re.findall(number, review_text)\n",
    "    if len(text_list) > 0:\n",
    "        temp.append(text_list)\n",
    "\n",
    "print(f'Dataset contains {len(temp)} stand alone number.')\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 28005 stand alone number.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [1]\n",
       "1         [2]\n",
       "2     [3, 10]\n",
       "3    [15, 25]\n",
       "4        [10]\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## br, @"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:41.236233Z",
     "start_time": "2025-03-17T07:36:38.769188Z"
    }
   },
   "source": [
    "# Define a regex pattern for br (HTML break line) and @ (mention) tags\n",
    "number= r'\\bRT\\b|@[A-Za-z0-9_]+|\\bbr\\b'\n",
    "\n",
    "# Let's explore whether the dataset br mentions @.\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    text_list = re.findall(number, review_text)\n",
    "    if len(text_list) > 0:\n",
    "        temp.append(text_list)\n",
    "\n",
    "print(f'Dataset contains {len(temp)} br, mentions @.')\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 29232 br, mentions @.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0            [br, br, br, br, br, br]\n",
       "1            [br, br, br, br, br, br]\n",
       "2                    [br, br, br, br]\n",
       "3            [br, br, br, br, br, br]\n",
       "4    [br, br, br, br, br, br, br, br]\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smiley"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:42.587309Z",
     "start_time": "2025-03-17T07:36:41.277595Z"
    }
   },
   "source": [
    "# Define a regex pattern for smileys\n",
    "smiley_pattern = r\"(:\\)|:-\\)|;d|:-E|:\\(|:-\\(|:-<|:-P|:O|:-@|:@|:-\\$|:\\\\|:#|:X|:\\^\\)|:-&|\\$_\\$|@@|:-!|:-D|:-0|O\\.O|<\\(-_-\\)>|d\\[-_-\\]b|;\\)|O:-\\)|O\\*\\)|\\(:-D\\)|=\\^.\\^=|\\^_\\^|:-?P|:-?O|:-?3|:-?\\])\"\n",
    "\n",
    "# Check for smileys in the dataset\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    smiley_list = re.findall(smiley_pattern, review_text)\n",
    "    if smiley_list:\n",
    "        temp.append(smiley_list)\n",
    "\n",
    "print(f'Dataset contains {len(temp)} number of reviews with smileys.')\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 684 number of reviews with smileys.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [:)]\n",
       "1    [;)]\n",
       "2    [;)]\n",
       "3    [:3]\n",
       "4    [:3]\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:44.648174Z",
     "start_time": "2025-03-17T07:36:42.625081Z"
    }
   },
   "source": [
    "# Define a regex pattern for contractions\n",
    "contraction_pattern = r\"\\b\\w+'\\w+\\b\"\n",
    "\n",
    "# Check for contractions in the dataset\n",
    "temp = []\n",
    "for index, row in data.iterrows():\n",
    "    review_text = row['review']\n",
    "    contractions_found = re.findall(contraction_pattern, review_text)\n",
    "    if contractions_found:\n",
    "        temp.append(contractions_found)\n",
    "\n",
    "print(f\"Dataset contains {len(temp)} reviews with contractions.\")\n",
    "tmp = pd.Series(temp)\n",
    "tmp.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 43839 reviews with contractions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [you'll, wouldn't, doesn't, couldn't, who'll, ...\n",
       "1                              [master's, Halliwell's]\n",
       "2                                 [I'd, Woody's, I've]\n",
       "3                           [there's, there's, you're]\n",
       "4                   [Mattei's, Schnitzler's, Mattei's]\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks, like the dataset has well-balanced classes for building the sentiment analysis. \\\n",
    "In this section, we will prepare the dataset for building the ML models by removing the noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:44.691710Z",
     "start_time": "2025-03-17T07:36:44.688257Z"
    }
   },
   "source": [
    "# Convert the sentiment classes from categorical to numeric representation.\n",
    "data[\"sentiment\"] = data[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:44.732796Z",
     "start_time": "2025-03-17T07:36:44.729432Z"
    }
   },
   "source": [
    "# Let's verify the data after conversion.\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:36:45.444961Z",
     "start_time": "2025-03-17T07:36:44.784738Z"
    }
   },
   "source": [
    "# Save to new CSV with classes encoded\n",
    "data.to_csv(encoded_dataset_path, index=False)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the identified noise in the dataset.\n",
    "#### NOTE: Takes around 15 to 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T07:37:39.016826Z",
     "start_time": "2025-03-17T07:36:45.481327Z"
    }
   },
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define spaCy's stop words\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"she's\": \"she is\",\n",
    "    \"he's\": \"he is\", \"they're\": \"they are\", \"we're\": \"we are\", \"i've\": \"i have\",\n",
    "    \"you're\": \"you are\", \"they've\": \"they have\", \"i'd\": \"i would\", \"we'd\": \"we would\",\n",
    "    \"couldn't\": \"could not\", \"wouldn't\": \"would not\", \"shouldn't\": \"should not\",\n",
    "    \"don't\": \"do not\", \"haven't\": \"have not\", \"omg\": \"oh my god\",\n",
    "    \"aren't\": \"are not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\", \"isn't\": \"is not\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\", \"mightn't\": \"might not\", \"might've\": \"might have\",\n",
    "    \"mustn't\": \"must not\", \"must've\": \"must have\", \"needn't\": \"need not\",\n",
    "    \"o'clock\": \"of the clock\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "    \"there'd\": \"there would\", \"they'd\": \"they would\", \"they'll\": \"they will\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"would've\": \"would have\", \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\", \"you've\": \"you have\", \"y'all\": \"you all\"\n",
    "}\n",
    "\n",
    "# Emojis dictionary\n",
    "emojis_dict = {\n",
    "    ':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',\n",
    "    ':-(': 'sad', ':-<': 'sad', ':-P': 'raspberry', ':O': 'surprised',\n",
    "    ':-@': 'shocked', ':@': 'shocked', ':-$': 'confused', r':\\\\': 'annoyed',\n",
    "    ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "    '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.O': 'confused',\n",
    "    '<(-_-)>': 'robot', 'd[-_-]b': 'dj', ':-)': 'sadsmile', ';)': 'wink',\n",
    "    'O:-)': 'angel', 'O*)': 'angel', '(:-D': 'gossip', '=^.^=': 'cat'\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1️⃣ Text Cleaning\n",
    "    text = re.sub(r'https?:\\/\\/[\\S]+', '', text)                # Remove hyperlinks\n",
    "    text = re.sub(r'[#,.\\-$!/()?%_√Ø¬ø¬Ω:&|;]', ' ', text)      # Replace special characters with a space\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)                  # Remove mentions\n",
    "    text = re.sub(r'\\d{1,4}[\\/\\-]\\d{1,2}[\\/\\-]\\d{1,4}', '', text) # Remove dates (e.g. 12/24/03)\n",
    "    text = re.sub(r'\\d+', '', text)                             # Remove standalone numbers\n",
    "    text = re.sub(r'\\bbr\\b', '', text)                          # Remove noise \"br\"\n",
    "\n",
    "\n",
    "    # 2️⃣ Contraction Expansion\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', full_form, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 3️⃣ Emoji Replacement\n",
    "    for emoji, meaning in emojis_dict.items():\n",
    "        text = text.replace(emoji, f' {meaning} ')\n",
    "    \n",
    "    # 4️⃣ Normalisation: convert to lowercase and remove extra spaces\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 5️⃣ Tokenization, Lemmatisation, and Stopword Removal using spaCy\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Read the CSV with encoding specified\n",
    "data = pd.read_csv(encoded_dataset_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Apply preprocessing to the \"review\" column\n",
    "data[\"review\"] = data[\"review\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Display the processed dataset\n",
    "print(\"\\nProcessed Dataset:\")\n",
    "print(data.head().to_string())\n",
    "\n",
    "# Save the processed dataset into CSV\n",
    "data.to_csv(processed_dataset_path, index=False)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 73\u001B[0m\n\u001B[1;32m     70\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(encoded_dataset_path, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mISO-8859-1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Apply preprocessing to the \"review\" column\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreview\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreview\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocess_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# Display the processed dataset\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mProcessed Dataset:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/pandas/core/series.py:4917\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[19], line 63\u001B[0m, in \u001B[0;36mpreprocess_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     60\u001B[0m text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms+\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m, text)\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# 5️⃣ Tokenization, Lemmatisation, and Stopword Removal using spaCy\u001B[39;00m\n\u001B[0;32m---> 63\u001B[0m doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m lemmatized_words \u001B[38;5;241m=\u001B[39m [token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc]\n\u001B[1;32m     65\u001B[0m filtered_words \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m lemmatized_words \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words \u001B[38;5;129;01mand\u001B[39;00m word\u001B[38;5;241m.\u001B[39misalpha()]\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/spacy/language.py:1052\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1050\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1052\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1054\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/spacy/pipeline/tok2vec.py:126\u001B[0m, in \u001B[0;36mTok2Vec.predict\u001B[0;34m(self, docs)\u001B[0m\n\u001B[1;32m    124\u001B[0m     width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mget_dim(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnO\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39malloc((\u001B[38;5;241m0\u001B[39m, width)) \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[0;32m--> 126\u001B[0m tokvecs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokvecs\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:334\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[1;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/with_array.py:42\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, Xseq, is_train)\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](Xseq, is_train)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], \u001B[43m_list_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/with_array.py:77\u001B[0m, in \u001B[0;36m_list_forward\u001B[0;34m(model, Xs, is_train)\u001B[0m\n\u001B[1;32m     75\u001B[0m lengths \u001B[38;5;241m=\u001B[39m NUMPY_OPS\u001B[38;5;241m.\u001B[39masarray1i([\u001B[38;5;28mlen\u001B[39m(seq) \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m Xs])\n\u001B[1;32m     76\u001B[0m Xf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(Xs, pad\u001B[38;5;241m=\u001B[39mpad)\n\u001B[0;32m---> 77\u001B[0m Yf, get_dXf \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mbackprop\u001B[39m(dYs: ListXd) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ListXd:\n\u001B[1;32m     80\u001B[0m     dYf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(dYs, pad\u001B[38;5;241m=\u001B[39mpad)\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/residual.py:41\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m d_output \u001B[38;5;241m+\u001B[39m dX\n\u001B[0;32m---> 41\u001B[0m Y, backprop_layer \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(X, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [X[i] \u001B[38;5;241m+\u001B[39m Y[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(X))], backprop\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "    \u001B[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/python-ws/imdb-sentiment-analysis/venv/lib/python3.9/site-packages/thinc/layers/maxout.py:52\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     50\u001B[0m W \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m W \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape2f(W, nO \u001B[38;5;241m*\u001B[39m nP, nI)\n\u001B[0;32m---> 52\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m Y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape1f(b, nO \u001B[38;5;241m*\u001B[39m nP)\n\u001B[1;32m     54\u001B[0m Z \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape3f(Y, Y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], nO, nP)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Exploratory data analysis after pre-processing"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's glance at the dataset.\n",
    "data = pd.read_csv(processed_dataset_path, encoding=\"ISO-8859-1\")\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The dataset has the following rows and columns.\n",
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud - Positive Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter positive and negative sentiment reviews\n",
    "positive_text = \" \".join(data[data[\"sentiment\"] == 1][\"review\"])  # Positive sentiment\n",
    "\n",
    "# Create Word Clouds\n",
    "wordcloud_positive = WordCloud(width=1800, height=2000, background_color='black', colormap='Greens').generate(positive_text)\n",
    "\n",
    "# Plot Word Clouds\n",
    "plt.figure(figsize=(22, 18))\n",
    "\n",
    "# Positive Sentiment Word Cloud\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_positive, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Positive Sentiment Word Cloud\", fontsize=14)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud Negative Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter positive and negative sentiment reviews\n",
    "negative_text = \" \".join(data[data[\"sentiment\"] == 0][\"review\"])  # Negative sentiment\n",
    "\n",
    "# Create Word Clouds\n",
    "wordcloud_negative = WordCloud(width=1800, height=2000, background_color='black', colormap='Reds').generate(negative_text)\n",
    "\n",
    "# Plot Word Clouds\n",
    "plt.figure(figsize=(22, 18))\n",
    "\n",
    "# Negative Sentiment Word Cloud\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Negative Sentiment Word Cloud\", fontsize=14)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "The dataset had few noise, we have explored them in the EDA and preprocessing sections. \\\n",
    "Next, we will use the preprocessed `data/2-imdb-movie-review-processed.csv` dataset for building the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
